{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# DINO-Foresight Test Notebook (Trained Checkpoint)\n\nThis notebook is for quick qualitative testing after training:\n- load checkpoint\n- pull one OpenDV validation batch\n- run one-step prediction\n- optional multi-step rollout\n- optional RGB decode for visualization\n\nUpdate paths in the config cell first."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom pathlib import Path\nfrom types import SimpleNamespace\n\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\nfrom src.data import OpenDV_VideoData\nfrom src.dino_f import Dino_f\nfrom save_predicted_dino_features import add_missing_args, denormalize_images\n\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint('DEVICE =', DEVICE)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== Config =====\n# 1) Your trained checkpoint\nDINO_F_CKPT = '/cpfs/pengyu/DINO-Foresight/logs/dino_foresight_lowres_opendv_drift_noallgather/REPLACE_ME/checkpoints/last.ckpt'\n\n# 2) Data\nOPENDV_ROOT = '/pfs/pengyu/OpenDV-YouTube'\nOPENDV_LANG_ROOT = '/pfs/pengyu/OpenDV-YouTube-Language'\nLANG_CACHE_VAL = '/pfs/pengyu/OpenDV-YouTube-Language/mini_val_cache.json'\nOPENDV_LANG_FEAT_NAME = 'lang_clip_{start}_{end}.pt'\n\n# 3) Optional decoder (set to None to skip RGB decoding)\nDECODER_CKPT = None\nDECODER_TYPE = 'from_dino'  # 'from_dino' or 'from_feats'\n\n# 4) Eval settings\nBATCH_SIZE = 1\nNUM_WORKERS = 2\nROLLOUT_STEPS = 6\nMAX_CLIPS = None\n\n# Optional: offline DINO hub cache\nos.environ.setdefault('DINO_REPO', '/cpfs/pengyu/.cache/torch/hub/facebookresearch_dinov2_main')\n\nassert Path(DINO_F_CKPT).exists(), f'Checkpoint not found: {DINO_F_CKPT}'"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load model\nmodel = Dino_f.load_from_checkpoint(DINO_F_CKPT, strict=False, map_location='cpu').to(DEVICE)\nmodel.eval()\nmodel._init_feature_extractor()\nif model.dino_v2 is not None:\n    model.dino_v2 = model.dino_v2.to(DEVICE)\nif model.eva2clip is not None:\n    model.eva2clip = model.eva2clip.to(DEVICE)\nif model.sam is not None:\n    model.sam = model.sam.to(DEVICE)\n\nprint('feature_extractor:', model.args.feature_extractor)\nprint('img_size:', model.args.img_size)\nprint('sequence_length:', model.args.sequence_length)\nprint('use_language_condition:', getattr(model.args, 'use_language_condition', False))\nprint('use_precomputed_text:', getattr(model.args, 'use_precomputed_text', False))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Build OpenDV validation dataloader (aligned with checkpoint args)\nuse_lang_cond = bool(getattr(model.args, 'use_language_condition', False))\nuse_precomputed_text = bool(getattr(model.args, 'use_precomputed_text', False))\n\nargs = SimpleNamespace(\n    data_path=OPENDV_ROOT,\n    opendv_root=OPENDV_ROOT,\n    opendv_lang_root=OPENDV_LANG_ROOT,\n    opendv_use_lang_annos=bool(OPENDV_LANG_ROOT),\n    opendv_lang_cache_train=None,\n    opendv_lang_cache_val=LANG_CACHE_VAL,\n    opendv_use_lang_features=bool(use_lang_cond and use_precomputed_text),\n    opendv_return_language=bool(use_lang_cond and (not use_precomputed_text)),\n    opendv_lang_feat_name=OPENDV_LANG_FEAT_NAME,\n    opendv_video_dir=None,\n    opendv_max_clips=MAX_CLIPS,\n    sequence_length=getattr(model.args, 'sequence_length', 5),\n    img_size=tuple(getattr(model.args, 'img_size', (224, 448))),\n    batch_size=BATCH_SIZE,\n    num_workers=NUM_WORKERS,\n    num_workers_val=NUM_WORKERS,\n    eval_mode=True,\n    eval_midterm=False,\n    eval_modality=None,\n    use_language_condition=use_lang_cond,\n    use_precomputed_text=use_precomputed_text,\n)\nargs = add_missing_args(args, model.args)\nargs.feature_extractor = model.args.feature_extractor\nargs.dinov2_variant = getattr(model.args, 'dinov2_variant', 'vitb14_reg')\nargs.return_rgb_path = True\n\ndata = OpenDV_VideoData(arguments=args, subset='val', batch_size=BATCH_SIZE)\nloader = data.val_dataloader()\nbatch = next(iter(loader))\nprint('Loaded one val batch.')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def parse_batch(batch):\n    if not isinstance(batch, (list, tuple)):\n        return batch, None, None, None, None\n\n    frames = batch[0]\n    gt_img = None\n    text_tokens = None\n    text_mask = None\n    rgb_paths = None\n\n    for item in batch[1:]:\n        if torch.is_tensor(item):\n            if item.ndim == 4 and item.shape[1] == 3:\n                gt_img = item\n            elif item.ndim == 3:\n                text_tokens = item\n            elif item.ndim == 2:\n                text_mask = item\n        elif isinstance(item, (list, tuple)) and len(item) > 0 and isinstance(item[0], (str, Path)):\n            rgb_paths = [str(p) for p in item]\n\n    return frames, gt_img, text_tokens, text_mask, rgb_paths\n\n\ndef _prepare_text_tokens_for_model(model, text_tokens, text_mask, device):\n    if not getattr(model, 'use_language_condition', False):\n        return None, None\n    if text_tokens is None:\n        return None, None\n\n    text_tokens = text_tokens.to(device)\n    if hasattr(model, 'text_proj'):\n        in_dim = model.text_proj.in_features\n        out_dim = model.text_proj.out_features\n        if text_tokens.shape[-1] == in_dim:\n            text_tokens = text_tokens.to(dtype=model.text_proj.weight.dtype)\n            text_tokens = model.text_proj(text_tokens)\n        elif text_tokens.shape[-1] != out_dim:\n            raise ValueError(\n                f'Unexpected text token dim {text_tokens.shape[-1]} (expected {in_dim} or {out_dim}).'\n            )\n    if text_mask is not None:\n        text_mask = text_mask.to(device)\n    return text_tokens, text_mask\n\n\n@torch.no_grad()\ndef one_step_predict_feats(model, frames, text_tokens=None, text_mask=None):\n    x = model.preprocess(frames)\n    text_tokens, text_mask = _prepare_text_tokens_for_model(model, text_tokens, text_mask, x.device)\n\n    masked_x, mask = model.get_mask_tokens(x, mode='full_mask', mask_frames=1)\n    mask = mask.to(x.device)\n\n    if model.args.vis_attn:\n        _, x_pred, _ = model.forward(x, masked_x, mask, text_tokens=text_tokens, text_mask=text_mask)\n    else:\n        _, x_pred = model.forward(x, masked_x, mask, text_tokens=text_tokens, text_mask=text_mask)\n\n    x_pred = model.postprocess(x_pred)\n    return x_pred[:, -1]  # [B, Hf, Wf, C]\n\n\n@torch.no_grad()\ndef rollout_predict_feats(model, frames, steps, text_tokens=None, text_mask=None):\n    preds = []\n    x = model.preprocess(frames)\n    text_tokens, text_mask = _prepare_text_tokens_for_model(model, text_tokens, text_mask, x.device)\n\n    for _ in range(steps):\n        masked_x, mask = model.get_mask_tokens(x, mode='full_mask', mask_frames=1)\n        mask = mask.to(x.device)\n\n        if model.args.vis_attn:\n            _, x_pred, _ = model.forward(x, masked_x, mask, text_tokens=text_tokens, text_mask=text_mask)\n        else:\n            _, x_pred = model.forward(x, masked_x, mask, text_tokens=text_tokens, text_mask=text_mask)\n\n        x_pred = model.postprocess(x_pred)\n        preds.append(x_pred[:, -1].detach().cpu())\n\n        # shift context and append latest prediction\n        x[:, :-1] = x[:, 1:].clone()\n        x[:, -1] = x_pred[:, -1]\n\n    return torch.stack(preds, dim=1)  # [B, S, Hf, Wf, C]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Parse batch and run one-step\nframes, gt_img, text_tokens, text_mask, rgb_paths = parse_batch(batch)\nframes = frames.to(DEVICE)\nif gt_img is not None:\n    gt_img = gt_img.to(DEVICE)\n\npred_feats = one_step_predict_feats(model, frames, text_tokens=text_tokens, text_mask=text_mask)\nprint('pred_feats shape:', tuple(pred_feats.shape))\n\n# Build gt feature for quick feature-space MSE\nif gt_img is not None:\n    with torch.no_grad():\n        gt_feats = model.extract_features(gt_img)\n        h = frames.shape[-2] // model.patch_size\n        w = frames.shape[-1] // model.patch_size\n        gt_feats = gt_feats.reshape(gt_feats.shape[0], h, w, -1)\n        feat_mse = F.mse_loss(pred_feats, gt_feats)\n    print('feature MSE (pred vs gt):', float(feat_mse))\nelse:\n    gt_feats = None\n    print('gt_img is None, skip feature MSE')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Optional decoder for RGB visualization\ndecode_feats = None\nif DECODER_CKPT is not None:\n    if DECODER_TYPE == 'from_feats':\n        from train_rgb_decoder_from_feats import FeatureRgbDecoder\n        decoder = FeatureRgbDecoder.load_from_checkpoint(DECODER_CKPT, strict=False).to(DEVICE)\n        decoder.eval()\n        def decode_feats(feats_bhwc):\n            return decoder(feats_bhwc.to(DEVICE)).detach().cpu()\n    elif DECODER_TYPE == 'from_dino':\n        from train_rgb_decoder import DinoV2RGBDecoder\n        decoder = DinoV2RGBDecoder.load_from_checkpoint(DECODER_CKPT, strict=False, lpips_weight=0).to(DEVICE)\n        decoder.eval()\n        dpt_head = decoder.decoder\n        feat_dim = decoder.emb_dim\n\n        def decode_feats(feats_bhwc):\n            feats_bhwc = feats_bhwc.to(DEVICE)\n            b, h, w, c = feats_bhwc.shape\n            if c % feat_dim != 0:\n                raise ValueError(f'Feature dim mismatch: {c} not divisible by {feat_dim}')\n            parts = torch.split(feats_bhwc, feat_dim, dim=-1)\n            if len(parts) == 2:\n                parts = [parts[0], parts[0], parts[1], parts[1]]\n            elif len(parts) != 4:\n                raise ValueError(f'Expected 2 or 4 feature chunks, got {len(parts)}')\n            feat_list = [p.reshape(b, h * w, feat_dim) for p in parts]\n            pred = dpt_head(feat_list, h, w)\n            pred = F.interpolate(pred, size=tuple(model.args.img_size), mode='bicubic', align_corners=False)\n            pred = torch.sigmoid(pred)\n            return pred.detach().cpu()\n    else:\n        raise ValueError(\"DECODER_TYPE must be 'from_dino' or 'from_feats'\")\n\nprint('decoder ready:', decode_feats is not None)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize context + one-step prediction (+ gt if available)\nctx = denormalize_images(frames[0].detach().cpu(), model.args.feature_extractor).cpu()  # [T, 3, H, W]\n\nshow_imgs = []\nshow_titles = []\n\nshow_imgs.append(ctx[-1])\nshow_titles.append('context last')\n\nif gt_img is not None:\n    gt_rgb = denormalize_images(gt_img.detach().cpu(), model.args.feature_extractor).cpu()[0]\n    show_imgs.append(gt_rgb)\n    show_titles.append('gt next')\n\nif decode_feats is not None:\n    pred_rgb = decode_feats(pred_feats)[0].clamp(0, 1)\n    show_imgs.append(pred_rgb)\n    show_titles.append('pred next')\nelse:\n    # fallback: visualize feature norm map\n    feat_norm = pred_feats[0].norm(dim=-1).detach().cpu()\n    plt.figure(figsize=(4, 3))\n    plt.imshow(feat_norm)\n    plt.title('pred feature norm map')\n    plt.axis('off')\n    plt.show()\n\nif len(show_imgs) > 0:\n    plt.figure(figsize=(4 * len(show_imgs), 4))\n    for i, (img, title) in enumerate(zip(show_imgs, show_titles), start=1):\n        plt.subplot(1, len(show_imgs), i)\n        plt.imshow(img.permute(1, 2, 0))\n        plt.title(title)\n        plt.axis('off')\n    plt.tight_layout()\n    plt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Optional: rollout visualization\npreds_rollout = rollout_predict_feats(model, frames, ROLLOUT_STEPS, text_tokens=text_tokens, text_mask=text_mask)\nprint('preds_rollout shape:', tuple(preds_rollout.shape))\n\nif decode_feats is None:\n    print('Set DECODER_CKPT to visualize RGB rollout.')\nelse:\n    cols = ROLLOUT_STEPS\n    plt.figure(figsize=(3 * cols, 3))\n    for t in range(ROLLOUT_STEPS):\n        rgb_t = decode_feats(preds_rollout[:, t])[0].clamp(0, 1)\n        ax = plt.subplot(1, cols, t + 1)\n        ax.imshow(rgb_t.permute(1, 2, 0))\n        ax.set_title(f'pred t+{t+1}')\n        ax.axis('off')\n    plt.tight_layout()\n    plt.show()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
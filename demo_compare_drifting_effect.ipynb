{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drifting Effect Test (A/B Comparison)\n",
    "\n",
    "Compare **drifting model** vs **non-drifting baseline** on the same OpenDV validation batches.\n",
    "\n",
    "Outputs:\n",
    "- one-step feature MSE (pred feature vs GT feature)\n",
    "- one-step feature cosine similarity\n",
    "- optional RGB PSNR/SSIM (if decoder is provided)\n",
    "- qualitative side-by-side visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.data import OpenDV_VideoData\n",
    "from src.dino_f import Dino_f\n",
    "from save_predicted_dino_features import add_missing_args, denormalize_images\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('DEVICE =', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72d44ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Config =====\n",
    "# Required: two checkpoints trained with same backbone/data settings\n",
    "CKPT_DRIFT = '/cpfs/pengyu/DINO-Foresight/logs/dino_foresight_lowres_opendv_drift_noallgather/REPLACE_ME/checkpoints/last.ckpt'\n",
    "CKPT_BASELINE = '/cpfs/pengyu/DINO-Foresight/logs/dino_foresight_lowres_opendv_baseline/REPLACE_ME/checkpoints/last.ckpt'\n",
    "\n",
    "# Data\n",
    "OPENDV_ROOT = '/pfs/pengyu/OpenDV-YouTube'\n",
    "OPENDV_LANG_ROOT = '/pfs/pengyu/OpenDV-YouTube-Language'\n",
    "LANG_CACHE_VAL = '/pfs/pengyu/OpenDV-YouTube-Language/mini_val_cache.json'\n",
    "OPENDV_LANG_FEAT_NAME = 'lang_clip_{start}_{end}.pt'\n",
    "\n",
    "# Eval setup\n",
    "BATCH_SIZE = 1\n",
    "NUM_WORKERS = 2\n",
    "NUM_EVAL_BATCHES = 50\n",
    "ROLLOUT_STEPS = 6  # qualitative only\n",
    "\n",
    "# Optional decoder for RGB metrics/visualization\n",
    "DECODER_CKPT = None\n",
    "DECODER_TYPE = 'from_dino'  # 'from_dino' or 'from_feats'\n",
    "SAVE_VIS_DIR = None  # e.g. './tmp_decode_vis' to save decoded RGB images\n",
    "\n",
    "# Optional: offline DINOv2 hub path\n",
    "os.environ.setdefault('DINO_REPO', '/cpfs/pengyu/.cache/torch/hub/facebookresearch_dinov2_main')\n",
    "\n",
    "assert Path(CKPT_DRIFT).exists(), f'Missing CKPT_DRIFT: {CKPT_DRIFT}'\n",
    "assert Path(CKPT_BASELINE).exists(), f'Missing CKPT_BASELINE: {CKPT_BASELINE}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309fc3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(ckpt_path):\n",
    "    model = Dino_f.load_from_checkpoint(ckpt_path, strict=False, map_location='cpu').to(DEVICE)\n",
    "    model.eval()\n",
    "    model._init_feature_extractor()\n",
    "    if model.dino_v2 is not None:\n",
    "        model.dino_v2 = model.dino_v2.to(DEVICE)\n",
    "    if model.eva2clip is not None:\n",
    "        model.eva2clip = model.eva2clip.to(DEVICE)\n",
    "    if model.sam is not None:\n",
    "        model.sam = model.sam.to(DEVICE)\n",
    "    return model\n",
    "\n",
    "model_drift = load_model(CKPT_DRIFT)\n",
    "model_base = load_model(CKPT_BASELINE)\n",
    "\n",
    "print('drift use_drifting_loss:', getattr(model_drift.args, 'use_drifting_loss', False))\n",
    "print('base  use_drifting_loss:', getattr(model_base.args, 'use_drifting_loss', False))\n",
    "print('drift feature_extractor:', model_drift.args.feature_extractor)\n",
    "print('base  feature_extractor:', model_base.args.feature_extractor)\n",
    "print('drift img_size:', model_drift.args.img_size)\n",
    "print('base  img_size:', model_base.args.img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1cc630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build one shared validation loader (use drift model args as source of truth)\n",
    "m = model_drift\n",
    "use_lang_cond = bool(getattr(m.args, 'use_language_condition', False))\n",
    "use_precomputed_text = bool(getattr(m.args, 'use_precomputed_text', False))\n",
    "\n",
    "args = SimpleNamespace(\n",
    "    data_path=OPENDV_ROOT,\n",
    "    opendv_root=OPENDV_ROOT,\n",
    "    opendv_lang_root=OPENDV_LANG_ROOT,\n",
    "    opendv_use_lang_annos=bool(OPENDV_LANG_ROOT),\n",
    "    opendv_lang_cache_train=None,\n",
    "    opendv_lang_cache_val=LANG_CACHE_VAL,\n",
    "    opendv_use_lang_features=bool(use_lang_cond and use_precomputed_text),\n",
    "    opendv_return_language=bool(use_lang_cond and (not use_precomputed_text)),\n",
    "    opendv_lang_feat_name=OPENDV_LANG_FEAT_NAME,\n",
    "    opendv_video_dir=None,\n",
    "    opendv_max_clips=None,\n",
    "    sequence_length=getattr(m.args, 'sequence_length', 5),\n",
    "    img_size=tuple(getattr(m.args, 'img_size', (224, 448))),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    num_workers_val=NUM_WORKERS,\n",
    "    eval_mode=True,\n",
    "    eval_midterm=False,\n",
    "    eval_modality=None,\n",
    "    use_language_condition=use_lang_cond,\n",
    "    use_precomputed_text=use_precomputed_text,\n",
    ")\n",
    "args = add_missing_args(args, m.args)\n",
    "args.feature_extractor = m.args.feature_extractor\n",
    "args.dinov2_variant = getattr(m.args, 'dinov2_variant', 'vitb14_reg')\n",
    "args.return_rgb_path = True\n",
    "\n",
    "data = OpenDV_VideoData(arguments=args, subset='val', batch_size=BATCH_SIZE)\n",
    "loader = data.val_dataloader()\n",
    "print('loader ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f01f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_batch(batch):\n",
    "    if not isinstance(batch, (list, tuple)):\n",
    "        return batch, None, None, None, None\n",
    "\n",
    "    frames = batch[0]\n",
    "    gt_img = None\n",
    "    text_tokens = None\n",
    "    text_mask = None\n",
    "    rgb_paths = None\n",
    "\n",
    "    for item in batch[1:]:\n",
    "        if torch.is_tensor(item):\n",
    "            if item.ndim == 4 and item.shape[1] == 3:\n",
    "                gt_img = item\n",
    "            elif item.ndim == 3:\n",
    "                text_tokens = item\n",
    "            elif item.ndim == 2:\n",
    "                text_mask = item\n",
    "        elif isinstance(item, (list, tuple)) and len(item) > 0 and isinstance(item[0], (str, Path)):\n",
    "            rgb_paths = [str(p) for p in item]\n",
    "\n",
    "    return frames, gt_img, text_tokens, text_mask, rgb_paths\n",
    "\n",
    "\n",
    "def prepare_text_for_model(model, text_tokens, text_mask, device):\n",
    "    if not getattr(model, 'use_language_condition', False):\n",
    "        return None, None\n",
    "    if text_tokens is None:\n",
    "        return None, None\n",
    "\n",
    "    tt = text_tokens.to(device)\n",
    "    tm = text_mask.to(device) if text_mask is not None else None\n",
    "    if hasattr(model, 'text_proj'):\n",
    "        in_dim = model.text_proj.in_features\n",
    "        out_dim = model.text_proj.out_features\n",
    "        if tt.shape[-1] == in_dim:\n",
    "            tt = tt.to(dtype=model.text_proj.weight.dtype)\n",
    "            tt = model.text_proj(tt)\n",
    "        elif tt.shape[-1] != out_dim:\n",
    "            raise ValueError(f'Unexpected text token dim {tt.shape[-1]} (expected {in_dim} or {out_dim})')\n",
    "    return tt, tm\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def one_step_pred_feats(model, frames, text_tokens=None, text_mask=None):\n",
    "    x = model.preprocess(frames)\n",
    "    tt, tm = prepare_text_for_model(model, text_tokens, text_mask, x.device)\n",
    "\n",
    "    masked_x, mask = model.get_mask_tokens(x, mode='full_mask', mask_frames=1)\n",
    "    mask = mask.to(x.device)\n",
    "    if model.args.vis_attn:\n",
    "        _, x_pred, _ = model.forward(x, masked_x, mask, text_tokens=tt, text_mask=tm)\n",
    "    else:\n",
    "        _, x_pred = model.forward(x, masked_x, mask, text_tokens=tt, text_mask=tm)\n",
    "\n",
    "    x_pred = model.postprocess(x_pred)\n",
    "    return x_pred[:, -1]\n",
    "\n",
    "\n",
    "def gt_next_feats(model, gt_img, h, w):\n",
    "    with torch.no_grad():\n",
    "        feats = model.extract_features(gt_img)\n",
    "        feats = feats.reshape(feats.shape[0], h, w, -1)\n",
    "    return feats\n",
    "\n",
    "\n",
    "def psnr(pred, gt):\n",
    "    mse = torch.mean((pred - gt) ** 2).clamp_min(1e-12)\n",
    "    return (10.0 * torch.log10(1.0 / mse)).item()\n",
    "\n",
    "\n",
    "def try_ssim(pred, gt):\n",
    "    try:\n",
    "        from torchmetrics.functional import structural_similarity_index_measure as ssim\n",
    "        return ssim(pred.unsqueeze(0), gt.unsqueeze(0), data_range=1.0).item()\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f211d533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional decoder\n",
    "decode_feats = None\n",
    "if DECODER_CKPT is not None:\n",
    "    if DECODER_TYPE == 'from_feats':\n",
    "        from train_rgb_decoder_from_feats import FeatureRgbDecoder\n",
    "        decoder = FeatureRgbDecoder.load_from_checkpoint(DECODER_CKPT, strict=False).to(DEVICE)\n",
    "        decoder.eval()\n",
    "        def decode_feats(feats_bhwc):\n",
    "            return decoder(feats_bhwc.to(DEVICE)).detach().cpu()\n",
    "    elif DECODER_TYPE == 'from_dino':\n",
    "        from train_rgb_decoder import DinoV2RGBDecoder\n",
    "        decoder = DinoV2RGBDecoder.load_from_checkpoint(DECODER_CKPT, strict=False, lpips_weight=0).to(DEVICE)\n",
    "        decoder.eval()\n",
    "        dpt_head = decoder.decoder\n",
    "        feat_dim = decoder.emb_dim\n",
    "\n",
    "        def decode_feats(feats_bhwc):\n",
    "            feats_bhwc = feats_bhwc.to(DEVICE)\n",
    "            b, h, w, c = feats_bhwc.shape\n",
    "            if c % feat_dim != 0:\n",
    "                raise ValueError(f'Feature dim mismatch: {c} not divisible by {feat_dim}')\n",
    "            parts = torch.split(feats_bhwc, feat_dim, dim=-1)\n",
    "            if len(parts) == 2:\n",
    "                parts = [parts[0], parts[0], parts[1], parts[1]]\n",
    "            elif len(parts) != 4:\n",
    "                raise ValueError(f'Expected 2 or 4 feature chunks, got {len(parts)}')\n",
    "            feat_list = [p.reshape(b, h * w, feat_dim) for p in parts]\n",
    "            pred = dpt_head(feat_list, h, w)\n",
    "            pred = F.interpolate(pred, size=tuple(model_drift.args.img_size), mode='bicubic', align_corners=False)\n",
    "            pred = torch.sigmoid(pred)\n",
    "            return pred.detach().cpu()\n",
    "    else:\n",
    "        raise ValueError(\"DECODER_TYPE must be 'from_dino' or 'from_feats'\")\n",
    "\n",
    "print('decoder enabled:', decode_feats is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df10341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantitative A/B test on NUM_EVAL_BATCHES\n",
    "mse_drift, mse_base = [], []\n",
    "cos_drift, cos_base = [], []\n",
    "psnr_drift, psnr_base = [], []\n",
    "ssim_drift, ssim_base = [], []\n",
    "\n",
    "last_sample = None\n",
    "\n",
    "for i, batch in enumerate(loader):\n",
    "    if i >= NUM_EVAL_BATCHES:\n",
    "        break\n",
    "\n",
    "    frames, gt_img, text_tokens, text_mask, rgb_paths = parse_batch(batch)\n",
    "    frames = frames.to(DEVICE)\n",
    "    if gt_img is None:\n",
    "        continue\n",
    "    gt_img = gt_img.to(DEVICE)\n",
    "\n",
    "    pred_d = one_step_pred_feats(model_drift, frames, text_tokens=text_tokens, text_mask=text_mask)\n",
    "    pred_b = one_step_pred_feats(model_base, frames, text_tokens=text_tokens, text_mask=text_mask)\n",
    "\n",
    "    h = frames.shape[-2] // model_drift.patch_size\n",
    "    w = frames.shape[-1] // model_drift.patch_size\n",
    "    gt_f = gt_next_feats(model_drift, gt_img, h, w)\n",
    "\n",
    "    mse_drift.append(F.mse_loss(pred_d, gt_f).item())\n",
    "    mse_base.append(F.mse_loss(pred_b, gt_f).item())\n",
    "\n",
    "    cos_drift.append(F.cosine_similarity(pred_d.reshape(pred_d.shape[0], -1), gt_f.reshape(gt_f.shape[0], -1), dim=-1).mean().item())\n",
    "    cos_base.append(F.cosine_similarity(pred_b.reshape(pred_b.shape[0], -1), gt_f.reshape(gt_f.shape[0], -1), dim=-1).mean().item())\n",
    "\n",
    "    if decode_feats is not None:\n",
    "        gt_rgb = denormalize_images(gt_img.detach().cpu(), model_drift.args.feature_extractor).cpu()[0].clamp(0, 1)\n",
    "        pd_rgb = decode_feats(pred_d)[0].clamp(0, 1)\n",
    "        pb_rgb = decode_feats(pred_b)[0].clamp(0, 1)\n",
    "\n",
    "        psnr_drift.append(psnr(pd_rgb, gt_rgb))\n",
    "        psnr_base.append(psnr(pb_rgb, gt_rgb))\n",
    "\n",
    "        sd = try_ssim(pd_rgb, gt_rgb)\n",
    "        sb = try_ssim(pb_rgb, gt_rgb)\n",
    "        if sd is not None and sb is not None:\n",
    "            ssim_drift.append(sd)\n",
    "            ssim_base.append(sb)\n",
    "\n",
    "    if i == 0:\n",
    "        last_sample = (frames.detach().cpu(), gt_img.detach().cpu(), pred_d.detach().cpu(), pred_b.detach().cpu())\n",
    "\n",
    "print('evaluated batches:', len(mse_drift))\n",
    "\n",
    "if len(mse_drift) == 0:\n",
    "    raise RuntimeError('No valid batches evaluated. Check val loader output format.')\n",
    "\n",
    "print('=== Feature-space metrics (lower MSE / higher cosine better) ===')\n",
    "print(f'MSE   drift: {np.mean(mse_drift):.6f} | base: {np.mean(mse_base):.6f} | delta(base-drift): {np.mean(mse_base)-np.mean(mse_drift):.6f}')\n",
    "print(f'Cos   drift: {np.mean(cos_drift):.6f} | base: {np.mean(cos_base):.6f} | delta(drift-base): {np.mean(cos_drift)-np.mean(cos_base):.6f}')\n",
    "\n",
    "if len(psnr_drift) > 0:\n",
    "    print('=== RGB metrics (higher better) ===')\n",
    "    print(f'PSNR  drift: {np.mean(psnr_drift):.3f} | base: {np.mean(psnr_base):.3f} | delta(drift-base): {np.mean(psnr_drift)-np.mean(psnr_base):.3f}')\n",
    "if len(ssim_drift) > 0:\n",
    "    print(f'SSIM  drift: {np.mean(ssim_drift):.4f} | base: {np.mean(ssim_base):.4f} | delta(drift-base): {np.mean(ssim_drift)-np.mean(ssim_base):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac8ff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qualitative one-step visualization (first evaluated batch)\n",
    "if last_sample is None:\n",
    "    print('No sample cached for visualization.')\n",
    "else:\n",
    "    frames_cpu, gt_img_cpu, pred_d_cpu, pred_b_cpu = last_sample\n",
    "    ctx_last = denormalize_images(frames_cpu[0], model_drift.args.feature_extractor).cpu()[-1]\n",
    "    gt_rgb = denormalize_images(gt_img_cpu, model_drift.args.feature_extractor).cpu()[0]\n",
    "\n",
    "    imgs = [ctx_last, gt_rgb]\n",
    "    titles = ['context last', 'gt next']\n",
    "\n",
    "    if decode_feats is not None:\n",
    "        pd_rgb = decode_feats(pred_d_cpu.to(DEVICE))[0].clamp(0, 1)\n",
    "        pb_rgb = decode_feats(pred_b_cpu.to(DEVICE))[0].clamp(0, 1)\n",
    "        imgs += [pd_rgb, pb_rgb]\n",
    "        titles += ['pred drift', 'pred baseline']\n",
    "\n",
    "        diff = (pd_rgb - pb_rgb).abs().mean(dim=0)\n",
    "        plt.figure(figsize=(4, 4))\n",
    "        plt.imshow(diff, cmap='magma')\n",
    "        plt.title('abs diff map: drift vs baseline')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    else:\n",
    "        # fallback feature-norm map\n",
    "        d_norm = pred_d_cpu[0].norm(dim=-1)\n",
    "        b_norm = pred_b_cpu[0].norm(dim=-1)\n",
    "        plt.figure(figsize=(8, 3))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(d_norm)\n",
    "        plt.title('drift pred feat norm')\n",
    "        plt.axis('off')\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(b_norm)\n",
    "        plt.title('base pred feat norm')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    if decode_feats is not None and SAVE_VIS_DIR is not None:\n",
    "        save_dir = Path(SAVE_VIS_DIR)\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        plt.imsave(save_dir / 'one_step_context_last.png', ctx_last.permute(1, 2, 0).cpu().numpy().clip(0, 1))\n",
    "        plt.imsave(save_dir / 'one_step_gt_next.png', gt_rgb.permute(1, 2, 0).cpu().numpy().clip(0, 1))\n",
    "        plt.imsave(save_dir / 'one_step_pred_drift.png', pd_rgb.permute(1, 2, 0).cpu().numpy().clip(0, 1))\n",
    "        plt.imsave(save_dir / 'one_step_pred_baseline.png', pb_rgb.permute(1, 2, 0).cpu().numpy().clip(0, 1))\n",
    "        plt.imsave(save_dir / 'one_step_abs_diff_mean.png', diff.cpu().numpy(), cmap='magma')\n",
    "        print(f'Saved one-step decoded images to: {save_dir}')\n",
    "\n",
    "    plt.figure(figsize=(4 * len(imgs), 4))\n",
    "    for i, (im, tt) in enumerate(zip(imgs, titles), start=1):\n",
    "        plt.subplot(1, len(imgs), i)\n",
    "        plt.imshow(im.permute(1, 2, 0))\n",
    "        plt.title(tt)\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3ac9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: qualitative rollout side-by-side (no GT for later steps)\n",
    "@torch.no_grad()\n",
    "def rollout(model, frames, steps, text_tokens=None, text_mask=None):\n",
    "    # Keep recurrent state in model's internal feature space (PCA space if enabled).\n",
    "    x = model.preprocess(frames)\n",
    "    tt, tm = prepare_text_for_model(model, text_tokens, text_mask, x.device)\n",
    "    outs = []\n",
    "    for _ in range(steps):\n",
    "        masked_x, mask = model.get_mask_tokens(x, mode='full_mask', mask_frames=1)\n",
    "        mask = mask.to(x.device)\n",
    "        if model.args.vis_attn:\n",
    "            _, x_pred_internal, _ = model.forward(x, masked_x, mask, text_tokens=tt, text_mask=tm)\n",
    "        else:\n",
    "            _, x_pred_internal = model.forward(x, masked_x, mask, text_tokens=tt, text_mask=tm)\n",
    "\n",
    "        # Save decoded/postprocessed prediction for visualization metrics.\n",
    "        x_pred_out = model.postprocess(x_pred_internal)\n",
    "        outs.append(x_pred_out[:, -1].detach().cpu())\n",
    "\n",
    "        # Recurrent update must stay in internal space to avoid PCA dim mismatch.\n",
    "        x[:, :-1] = x[:, 1:].clone()\n",
    "        x[:, -1] = x_pred_internal[:, -1]\n",
    "    return torch.stack(outs, dim=1)\n",
    "\n",
    "if last_sample is None or decode_feats is None:\n",
    "    print('Need cached sample + decoder to show rollout.')\n",
    "else:\n",
    "    frames_cpu, _, _, _ = last_sample\n",
    "    frames0 = frames_cpu.to(DEVICE)\n",
    "\n",
    "    # Re-parse the first batch text tokens to keep language condition aligned\n",
    "    batch0 = next(iter(loader))\n",
    "    frames_b, _, text_t, text_m, _ = parse_batch(batch0)\n",
    "    frames_b = frames_b.to(DEVICE)\n",
    "\n",
    "    rd = rollout(model_drift, frames_b, ROLLOUT_STEPS, text_tokens=text_t, text_mask=text_m)\n",
    "    rb = rollout(model_base, frames_b, ROLLOUT_STEPS, text_tokens=text_t, text_mask=text_m)\n",
    "\n",
    "    plt.figure(figsize=(3 * ROLLOUT_STEPS, 6))\n",
    "    for t in range(ROLLOUT_STEPS):\n",
    "        img_d = decode_feats(rd[:, t].to(DEVICE))[0].clamp(0, 1)\n",
    "        img_b = decode_feats(rb[:, t].to(DEVICE))[0].clamp(0, 1)\n",
    "\n",
    "        if SAVE_VIS_DIR is not None:\n",
    "            rollout_dir = Path(SAVE_VIS_DIR) / 'rollout'\n",
    "            rollout_dir.mkdir(parents=True, exist_ok=True)\n",
    "            plt.imsave(rollout_dir / f'drift_t{t+1}.png', img_d.permute(1, 2, 0).cpu().numpy().clip(0, 1))\n",
    "            plt.imsave(rollout_dir / f'base_t{t+1}.png', img_b.permute(1, 2, 0).cpu().numpy().clip(0, 1))\n",
    "\n",
    "        ax1 = plt.subplot(2, ROLLOUT_STEPS, t + 1)\n",
    "        ax1.imshow(img_d.permute(1, 2, 0))\n",
    "        ax1.set_title(f'drift t+{t+1}')\n",
    "        ax1.axis('off')\n",
    "\n",
    "        ax2 = plt.subplot(2, ROLLOUT_STEPS, ROLLOUT_STEPS + t + 1)\n",
    "        ax2.imshow(img_b.permute(1, 2, 0))\n",
    "        ax2.set_title(f'base t+{t+1}')\n",
    "        ax2.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    if SAVE_VIS_DIR is not None:\n",
    "        print(f\"Saved rollout decoded images to: {Path(SAVE_VIS_DIR) / 'rollout'}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
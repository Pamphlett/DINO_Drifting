{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "091750e3",
      "metadata": {},
      "source": [
        "# DINO-Foresight Demo: Long Rollout + Language Swap\n",
        "\n",
        "This notebook lets you:\n",
        "1. Run long rollout predictions and decode future frames.\n",
        "2. Swap language condition to test whether predictions change plausibly.\n",
        "\n",
        "Notes:\n",
        "- Fill in paths in the config cell.\n",
        "- You can use a decoder trained from raw images (`train_rgb_decoder.py`) or from features (`train_rgb_decoder_from_feats.py`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46c9365a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from types import SimpleNamespace\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from src.data import OpenDV_VideoData\n",
        "from src.dino_f import Dino_f\n",
        "from save_predicted_dino_features import add_missing_args, denormalize_images\n",
        "from train_rgb_decoder import DinoV2RGBDecoder\n",
        "from train_rgb_decoder_from_feats import FeatureRgbDecoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1171ceb2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===== Config =====\n",
        "DINO_F_CKPT_LANG = '/cpfs/pengyu/DINO-Foresight/dino-foresight/nbvdoouw/checkpoints/last.ckpt'\n",
        "DINO_F_CKPT_NOLANG = '/cpfs/pengyu/DINO-Foresight/dino-foresight/klyq45xm/checkpoints/last.ckpt'\n",
        "DECODER_CKPT = '/cpfs/pengyu/DINO-Foresight/dino-foresight/j5ludt8t/checkpoints/epoch=20-step=189609.ckpt'\n",
        "DECODER_TYPE = 'from_dino'  # 'from_dino' or 'from_feats'\n",
        "PCA_CKPT = '/cpfs/pengyu/DINO-Foresight/dinov2_pca_224_l[2,_5,_8,_11]_1152.pth'\n",
        "\n",
        "OPENDV_ROOT = '/pfs/pengyu/OpenDV-YouTube'\n",
        "OPENDV_LANG_ROOT = '/pfs/pengyu/OpenDV-YouTube-Language'\n",
        "LANG_CACHE = '/pfs/pengyu/OpenDV-YouTube-Language/mini_val_cache.json'\n",
        "OPENDV_LANG_FEAT_NAME = 'lang_clip_{start}_{end}.pt'\n",
        "\n",
        "SEQUENCE_LENGTH = 5\n",
        "IMG_SIZE = (196, 392)\n",
        "ROLLOUT_STEPS = 6  # adjust rollout steps here\n",
        "BATCH_SIZE = 1\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Optional: use DINO hub cache offline\n",
        "os.environ.setdefault('DINO_REPO', '/cpfs/pengyu/.cache/torch/hub/facebookresearch_dinov2_main')\n",
        "\n",
        "HF_HOME = os.environ.get('HF_HOME', '/cpfs/pengyu/hfcaches')  # contains hub/\n",
        "CLIP_CACHE_DIR = os.environ.get('CLIP_CACHE_DIR', HF_HOME)\n",
        "CLIP_LOCAL_ONLY = True\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d5303f2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load DINO-Foresight predictors (with / without language)\n",
        "model_lang = Dino_f.load_from_checkpoint(DINO_F_CKPT_LANG, strict=False, pca_ckpt=PCA_CKPT).to(DEVICE)\n",
        "model_lang.eval()\n",
        "model_lang._init_feature_extractor()\n",
        "if model_lang.dino_v2 is not None:\n",
        "    model_lang.dino_v2 = model_lang.dino_v2.to(DEVICE)\n",
        "if model_lang.eva2clip is not None:\n",
        "    model_lang.eva2clip = model_lang.eva2clip.to(DEVICE)\n",
        "if model_lang.sam is not None:\n",
        "    model_lang.sam = model_lang.sam.to(DEVICE)\n",
        "\n",
        "model_nolang = Dino_f.load_from_checkpoint(DINO_F_CKPT_NOLANG, strict=False, pca_ckpt=PCA_CKPT).to(DEVICE)\n",
        "model_nolang.eval()\n",
        "model_nolang._init_feature_extractor()\n",
        "if model_nolang.dino_v2 is not None:\n",
        "    model_nolang.dino_v2 = model_nolang.dino_v2.to(DEVICE)\n",
        "if model_nolang.eva2clip is not None:\n",
        "    model_nolang.eva2clip = model_nolang.eva2clip.to(DEVICE)\n",
        "if model_nolang.sam is not None:\n",
        "    model_nolang.sam = model_nolang.sam.to(DEVICE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d742d3c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build OpenDV dataloader with language features\n",
        "args = SimpleNamespace(\n",
        "    data_path=OPENDV_ROOT,\n",
        "    opendv_root=OPENDV_ROOT,\n",
        "    opendv_lang_root=OPENDV_LANG_ROOT,\n",
        "    opendv_use_lang_annos=True,\n",
        "    opendv_lang_cache_train=None,\n",
        "    opendv_lang_cache_val=LANG_CACHE,\n",
        "    opendv_use_lang_features=True,\n",
        "    opendv_lang_feat_name=OPENDV_LANG_FEAT_NAME,\n",
        "    opendv_video_dir=None,\n",
        "    sequence_length=SEQUENCE_LENGTH,\n",
        "    img_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=4,\n",
        "    subset='val',\n",
        "    eval_mode=True,\n",
        "    eval_midterm=False,\n",
        "    eval_modality=None,\n",
        "    use_language_condition=getattr(model_lang, 'use_language_condition', False),\n",
        ")\n",
        "args = add_missing_args(args, model_lang.args)\n",
        "args.feature_extractor = model_lang.args.feature_extractor\n",
        "args.dinov2_variant = getattr(model_lang.args, 'dinov2_variant', 'vitb14_reg')\n",
        "args.return_rgb_path = True\n",
        "\n",
        "data = OpenDV_VideoData(arguments=args, subset='val', batch_size=BATCH_SIZE)\n",
        "loader = data.val_dataloader()\n",
        "batch = next(iter(loader))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1a82641",
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_batch(batch):\n",
        "    if isinstance(batch, (list, tuple)):\n",
        "        frames = batch[0]\n",
        "        gt_img = None\n",
        "        text_tokens = None\n",
        "        text_mask = None\n",
        "        rgb_paths = None\n",
        "        for item in batch[1:]:\n",
        "            if not torch.is_tensor(item):\n",
        "                if isinstance(item, (list, tuple)) and item:\n",
        "                    first = item[0]\n",
        "                    if isinstance(first, (str, Path)):\n",
        "                        rgb_paths = [str(p) for p in item]\n",
        "                continue\n",
        "            if item.ndim == 4 and item.shape[1] == 3:\n",
        "                gt_img = item\n",
        "            elif item.ndim == 3:\n",
        "                text_tokens = item\n",
        "            elif item.ndim == 2:\n",
        "                text_mask = item\n",
        "        return frames, gt_img, text_tokens, text_mask, rgb_paths\n",
        "    return batch, None, None, None, None\n",
        "\n",
        "frames, gt_img, text_tokens, text_mask, rgb_paths = parse_batch(batch)\n",
        "frames = frames.to(DEVICE)\n",
        "if text_tokens is not None:\n",
        "    text_tokens = text_tokens.to(DEVICE)\n",
        "if text_mask is not None:\n",
        "    text_mask = text_mask.to(DEVICE)\n",
        "\n",
        "print('frames', tuple(frames.shape))\n",
        "print('text_tokens', None if text_tokens is None else tuple(text_tokens.shape))\n",
        "print('text_mask', None if text_mask is None else tuple(text_mask.shape))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d92deb3d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show GT image (if available) and last context frame\n",
        "ctx = denormalize_images(frames[0], model_lang.args.feature_extractor).cpu()\n",
        "last_ctx = ctx[-1]\n",
        "plt.figure(figsize=(6, 3))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(last_ctx.permute(1, 2, 0))\n",
        "plt.title('last context frame')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "if gt_img is not None:\n",
        "    gt_rgb = denormalize_images(gt_img, model_lang.args.feature_extractor).cpu()\n",
        "    plt.imshow(gt_rgb[0].permute(1, 2, 0))\n",
        "    plt.title('gt image')\n",
        "else:\n",
        "    plt.text(0.1, 0.5, 'gt_img is None', fontsize=12)\n",
        "    plt.title('gt image')\n",
        "plt.axis('off')\n",
        "plt.tight_layout()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f986094a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def rollout_predictions(model, frames, unroll_steps, text_tokens=None, text_mask=None):\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        x = model.preprocess(frames)\n",
        "        if not getattr(model, 'use_language_condition', False):\n",
        "            text_tokens = None\n",
        "            text_mask = None\n",
        "        if text_tokens is not None:\n",
        "            text_tokens = text_tokens.to(x.device)\n",
        "            if hasattr(model, 'text_proj'):\n",
        "                in_dim = model.text_proj.in_features\n",
        "                out_dim = model.text_proj.out_features\n",
        "                if text_tokens.shape[-1] == in_dim:\n",
        "                    text_tokens = text_tokens.to(dtype=model.text_proj.weight.dtype)\n",
        "                    text_tokens = model.text_proj(text_tokens)\n",
        "                elif text_tokens.shape[-1] != out_dim:\n",
        "                    raise ValueError(\n",
        "                        f'Unexpected text token dim {text_tokens.shape[-1]} (expected {in_dim} or {out_dim}).'\n",
        "                    )\n",
        "        if text_mask is not None:\n",
        "            text_mask = text_mask.to(x.device)\n",
        "        for _ in range(unroll_steps):\n",
        "            masked_x, mask = model.get_mask_tokens(x, mode='full_mask', mask_frames=1)\n",
        "            mask = mask.to(x.device)\n",
        "            if model.args.vis_attn:\n",
        "                _, final_tokens, _ = model.forward(\n",
        "                    x, masked_x, mask, text_tokens=text_tokens, text_mask=text_mask\n",
        "                )\n",
        "            else:\n",
        "                _, final_tokens = model.forward(\n",
        "                    x, masked_x, mask, text_tokens=text_tokens, text_mask=text_mask\n",
        "                )\n",
        "            # Roll the context: drop oldest frame, append new prediction\n",
        "            x[:, 0:-1] = x[:, 1:].clone()\n",
        "            x[:, -1] = final_tokens[:, -1]\n",
        "            pred_feats = model.postprocess(x)[:, -1]\n",
        "            preds.append(pred_feats.detach().cpu())\n",
        "    return torch.stack(preds, dim=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a62e8fda",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load decoder\n",
        "if DECODER_TYPE == 'from_feats':\n",
        "    decoder = FeatureRgbDecoder.load_from_checkpoint(DECODER_CKPT, strict=False).to(DEVICE)\n",
        "    decoder.eval()\n",
        "    def decode_feats(feats_bhwc):\n",
        "        return decoder(feats_bhwc.to(DEVICE)).detach().cpu()\n",
        "elif DECODER_TYPE == 'from_dino':\n",
        "    # Uses the DPT head trained with online DINO features.\n",
        "    decoder = DinoV2RGBDecoder.load_from_checkpoint(DECODER_CKPT, strict=False, lpips_weight=0).to(DEVICE)\n",
        "    decoder.eval()\n",
        "    dpt_head = decoder.decoder\n",
        "    feat_dim = decoder.emb_dim\n",
        "    def decode_feats(feats_bhwc):\n",
        "        feats_bhwc = feats_bhwc.to(DEVICE)\n",
        "        b, h, w, c = feats_bhwc.shape\n",
        "        if c % feat_dim != 0:\n",
        "            raise ValueError(f'Feature dim mismatch: {c} not divisible by {feat_dim}')\n",
        "        parts = torch.split(feats_bhwc, feat_dim, dim=-1)\n",
        "        if len(parts) == 2:\n",
        "            parts = [parts[0], parts[0], parts[1], parts[1]]\n",
        "        elif len(parts) != 4:\n",
        "            raise ValueError(f'Expected 2 or 4 feature chunks, got {len(parts)}')\n",
        "        feat_list = [p.reshape(b, h * w, feat_dim) for p in parts]\n",
        "        pred = dpt_head(feat_list, h, w)\n",
        "        pred = F.interpolate(pred, size=IMG_SIZE, mode='bicubic', align_corners=False)\n",
        "        pred = torch.sigmoid(pred)\n",
        "        return pred.detach().cpu()\n",
        "else:\n",
        "    raise ValueError('DECODER_TYPE must be from_feats or from_dino')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0858820",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Long rollout\n",
        "preds = rollout_predictions(model_lang, frames, ROLLOUT_STEPS, text_tokens=text_tokens, text_mask=text_mask)\n",
        "# preds: [B, S, H, W, C]\n",
        "pred_rgb = []\n",
        "for t in range(preds.shape[1]):\n",
        "    rgb = decode_feats(preds[:, t])  # [B,3,H,W]\n",
        "    pred_rgb.append(rgb[0])\n",
        "\n",
        "# Visualize context frames + rollout\n",
        "ctx = denormalize_images(frames[0], model_lang.args.feature_extractor).cpu()  # [T,3,H,W]\n",
        "num_ctx = ctx.shape[0]\n",
        "fig_cols = max(num_ctx + len(pred_rgb), 1)\n",
        "plt.figure(figsize=(3 * fig_cols, 3))\n",
        "for i in range(num_ctx):\n",
        "    plt.subplot(1, fig_cols, i + 1)\n",
        "    plt.imshow(ctx[i].permute(1, 2, 0))\n",
        "    plt.title(f'ctx {i}')\n",
        "    plt.axis('off')\n",
        "for j, rgb in enumerate(pred_rgb):\n",
        "    plt.subplot(1, fig_cols, num_ctx + j + 1)\n",
        "    plt.imshow(rgb.permute(1, 2, 0))\n",
        "    plt.title(f'pred {j+1}')\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53d72538",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare rollout WITH vs WITHOUT language condition\n",
        "preds_lang = rollout_predictions(model_lang, frames, ROLLOUT_STEPS, text_tokens=text_tokens, text_mask=text_mask)\n",
        "preds_nolang = rollout_predictions(model_nolang, frames, ROLLOUT_STEPS, text_tokens=None, text_mask=None)\n",
        "\n",
        "rgb_lang = decode_feats(preds_lang[:, -1])[0]\n",
        "rgb_nolang = decode_feats(preds_nolang[:, -1])[0]\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(rgb_lang.permute(1, 2, 0))\n",
        "plt.title('with language (last step)')\n",
        "plt.axis('off')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(rgb_nolang.permute(1, 2, 0))\n",
        "plt.title('no language (last step)')\n",
        "plt.axis('off')\n",
        "plt.tight_layout()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c97ce01",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quality metrics per step + average (if gt_img is available)\n",
        "def _psnr(pred, gt):\n",
        "    mse = torch.mean((pred - gt) ** 2).clamp_min(1e-12)\n",
        "    return (10.0 * torch.log10(1.0 / mse)).item()\n",
        "\n",
        "def _try_ssim(pred, gt):\n",
        "    try:\n",
        "        from torchmetrics.functional import structural_similarity_index_measure as ssim\n",
        "        return ssim(pred.unsqueeze(0), gt.unsqueeze(0), data_range=1.0).item()\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "if gt_img is None:\n",
        "    print('gt_img is None; skip metrics')\n",
        "else:\n",
        "    gt_rgb = denormalize_images(gt_img, model_lang.args.feature_extractor).cpu()[0]\n",
        "    psnr_lang_list, psnr_nolang_list = [], []\n",
        "    ssim_lang_list, ssim_nolang_list = [], []\n",
        "\n",
        "    for t in range(ROLLOUT_STEPS):\n",
        "        pred_lang_t = decode_feats(preds_lang[:, t])[0].clamp(0, 1).cpu()\n",
        "        pred_nolang_t = decode_feats(preds_nolang[:, t])[0].clamp(0, 1).cpu()\n",
        "\n",
        "        psnr_lang = _psnr(pred_lang_t, gt_rgb)\n",
        "        psnr_nolang = _psnr(pred_nolang_t, gt_rgb)\n",
        "        psnr_lang_list.append(psnr_lang)\n",
        "        psnr_nolang_list.append(psnr_nolang)\n",
        "\n",
        "        ssim_lang = _try_ssim(pred_lang_t, gt_rgb)\n",
        "        ssim_nolang = _try_ssim(pred_nolang_t, gt_rgb)\n",
        "        if ssim_lang is not None and ssim_nolang is not None:\n",
        "            ssim_lang_list.append(ssim_lang)\n",
        "            ssim_nolang_list.append(ssim_nolang)\n",
        "\n",
        "        print(f'step {t+1:02d} | PSNR lang {psnr_lang:.2f} dB | PSNR nolang {psnr_nolang:.2f} dB')\n",
        "        if ssim_lang is not None and ssim_nolang is not None:\n",
        "            print(f'          | SSIM lang {ssim_lang:.4f}    | SSIM nolang {ssim_nolang:.4f}')\n",
        "\n",
        "    print('--- average over steps ---')\n",
        "    print(f'PSNR lang   : {sum(psnr_lang_list)/len(psnr_lang_list):.2f} dB')\n",
        "    print(f'PSNR nolang : {sum(psnr_nolang_list)/len(psnr_nolang_list):.2f} dB')\n",
        "    if ssim_lang_list and ssim_nolang_list:\n",
        "        print(f'SSIM lang   : {sum(ssim_lang_list)/len(ssim_lang_list):.4f}')\n",
        "        print(f'SSIM nolang : {sum(ssim_nolang_list)/len(ssim_nolang_list):.4f}')\n",
        "    else:\n",
        "        print('SSIM not available (torchmetrics not installed)')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2307256",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare full rollout (all steps) between lang and no-lang models\n",
        "preds_lang = rollout_predictions(model_lang, frames, ROLLOUT_STEPS, text_tokens=text_tokens, text_mask=text_mask)\n",
        "preds_nolang = rollout_predictions(model_nolang, frames, ROLLOUT_STEPS, text_tokens=None, text_mask=None)\n",
        "\n",
        "rgb_lang = [decode_feats(preds_lang[:, t])[0] for t in range(preds_lang.shape[1])]\n",
        "rgb_nolang = [decode_feats(preds_nolang[:, t])[0] for t in range(preds_nolang.shape[1])]\n",
        "\n",
        "cols = ROLLOUT_STEPS\n",
        "plt.figure(figsize=(3 * cols, 6))\n",
        "for t in range(ROLLOUT_STEPS):\n",
        "    plt.subplot(2, cols, t + 1)\n",
        "    plt.imshow(rgb_lang[t].permute(1, 2, 0))\n",
        "    plt.title(f'lang t{t+1}')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(2, cols, cols + t + 1)\n",
        "    plt.imshow(rgb_nolang[t].permute(1, 2, 0))\n",
        "    plt.title(f'nolang t{t+1}')\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af7c2bec",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Per-step comparison with GT (lang + nolang)\n",
        "if gt_img is None:\n",
        "    print('gt_img is None; skip GT comparison')\n",
        "else:\n",
        "    gt_rgb = denormalize_images(gt_img, model_lang.args.feature_extractor).cpu()[0]\n",
        "    cols = ROLLOUT_STEPS\n",
        "    plt.figure(figsize=(3 * cols, 9))\n",
        "    for t in range(ROLLOUT_STEPS):\n",
        "        # Row 1: GT (same for all steps)\n",
        "        ax = plt.subplot(3, cols, t + 1)\n",
        "        ax.imshow(gt_rgb.permute(1, 2, 0))\n",
        "        if t == 0:\n",
        "            ax.set_ylabel('GT', rotation=0, labelpad=40, fontsize=10)\n",
        "        ax.set_title(f't{t+1}')\n",
        "        ax.axis('off')\n",
        "\n",
        "        # Row 2: lang\n",
        "        ax = plt.subplot(3, cols, cols + t + 1)\n",
        "        try:\n",
        "            rgb_lang_t = rgb_lang[t]\n",
        "        except Exception:\n",
        "            rgb_lang_t = decode_feats(preds_lang[:, t])[0]\n",
        "        ax.imshow(rgb_lang_t.permute(1, 2, 0))\n",
        "        if t == 0:\n",
        "            ax.set_ylabel('lang', rotation=0, labelpad=40, fontsize=10)\n",
        "        ax.axis('off')\n",
        "\n",
        "        # Row 3: nolang\n",
        "        ax = plt.subplot(3, cols, 2 * cols + t + 1)\n",
        "        try:\n",
        "            rgb_nolang_t = rgb_nolang[t]\n",
        "        except Exception:\n",
        "            rgb_nolang_t = decode_feats(preds_nolang[:, t])[0]\n",
        "        ax.imshow(rgb_nolang_t.permute(1, 2, 0))\n",
        "        if t == 0:\n",
        "            ax.set_ylabel('nolang', rotation=0, labelpad=40, fontsize=10)\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee8feebc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Language condition swap\n",
        "# Take another batch and swap text tokens to see effect\n",
        "batch_b = next(iter(loader))\n",
        "frames_b, gt_b, text_b, mask_b, _ = parse_batch(batch_b)\n",
        "frames_b = frames_b.to(DEVICE)\n",
        "if text_b is not None:\n",
        "    text_b = text_b.to(DEVICE)\n",
        "if mask_b is not None:\n",
        "    mask_b = mask_b.to(DEVICE)\n",
        "\n",
        "preds_a = rollout_predictions(model_lang, frames, ROLLOUT_STEPS, text_tokens=text_tokens, text_mask=text_mask)\n",
        "preds_swap = rollout_predictions(model_lang, frames, ROLLOUT_STEPS, text_tokens=text_b, text_mask=mask_b)\n",
        "\n",
        "rgb_a = decode_feats(preds_a[:, -1])[0]\n",
        "rgb_swap = decode_feats(preds_swap[:, -1])[0]\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(rgb_a.permute(1, 2, 0))\n",
        "plt.title('lang A (last step)')\n",
        "plt.axis('off')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(rgb_swap.permute(1, 2, 0))\n",
        "plt.title('lang B (last step)')\n",
        "plt.axis('off')\n",
        "plt.tight_layout()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a5b76e6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prompt-based rollout (online CLIP embeddings)\n",
        "from transformers import CLIPTokenizer, CLIPTextModel\n",
        "import os.path as osp\n",
        "\n",
        "def _resolve_clip_sources(model_id, cache_dir, local_only):\n",
        "    if osp.isdir(model_id):\n",
        "        return model_id, model_id\n",
        "    offline = local_only or os.environ.get('HF_HUB_OFFLINE') or os.environ.get('TRANSFORMERS_OFFLINE')\n",
        "    if not offline:\n",
        "        return model_id, model_id\n",
        "    cache_root = cache_dir or os.environ.get('HF_HOME')\n",
        "    if cache_root and osp.basename(cache_root.rstrip('/')) != 'hub':\n",
        "        if osp.isdir(osp.join(cache_root, 'hub')):\n",
        "            cache_root = osp.join(cache_root, 'hub')\n",
        "    if not cache_root:\n",
        "        return model_id, model_id\n",
        "    model_dir = osp.join(cache_root, 'models--' + model_id.replace('/', '--'))\n",
        "    snapshots_dir = osp.join(model_dir, 'snapshots')\n",
        "    if not osp.isdir(snapshots_dir):\n",
        "        return model_id, model_id\n",
        "    snapshot_paths = [\n",
        "        osp.join(snapshots_dir, name)\n",
        "        for name in os.listdir(snapshots_dir)\n",
        "        if osp.isdir(osp.join(snapshots_dir, name))\n",
        "    ]\n",
        "    tokenizer_path = None\n",
        "    text_model_path = None\n",
        "    for snap in snapshot_paths:\n",
        "        if tokenizer_path is None and (\n",
        "            osp.isfile(osp.join(snap, 'tokenizer.json'))\n",
        "            or osp.isfile(osp.join(snap, 'vocab.json'))\n",
        "        ):\n",
        "            tokenizer_path = snap\n",
        "        if text_model_path is None and osp.isfile(osp.join(snap, 'model.safetensors')):\n",
        "            text_model_path = snap\n",
        "    return tokenizer_path or model_id, text_model_path or model_id\n",
        "\n",
        "clip_model_name = getattr(model_lang.args, 'clip_model_name', 'openai/clip-vit-base-patch32')\n",
        "clip_cache_dir = CLIP_CACHE_DIR\n",
        "clip_local_only = CLIP_LOCAL_ONLY\n",
        "clip_max_length = getattr(model_lang.args, 'clip_max_length', 77)\n",
        "\n",
        "tok_src, txt_src = _resolve_clip_sources(clip_model_name, clip_cache_dir, clip_local_only)\n",
        "tokenizer = CLIPTokenizer.from_pretrained(tok_src, cache_dir=clip_cache_dir, local_files_only=clip_local_only)\n",
        "text_model = CLIPTextModel.from_pretrained(txt_src, use_safetensors=True, cache_dir=clip_cache_dir, local_files_only=clip_local_only)\n",
        "text_model.eval().to(DEVICE)\n",
        "\n",
        "plain_caption_dict = {\n",
        "    0: 'Go straight.',\n",
        "    1: 'Pass the intersection.',\n",
        "    2: 'Turn left.',\n",
        "    3: 'Turn right.',\n",
        "    4: 'Change to the left lane.',\n",
        "    5: 'Change to the right lane.',\n",
        "    6: 'Go to the left lane branch.',\n",
        "    7: 'Go to the right lane branch.',\n",
        "    8: 'Pass the crosswalk.',\n",
        "    9: 'Pass the railroad.',\n",
        "    10: 'Merge.',\n",
        "    11: 'Make a U-turn.',\n",
        "    12: 'Stop.',\n",
        "    13: 'Deviate.',\n",
        "}\n",
        "\n",
        "SELECTED_CMDS = [0, 2, 11]  # choose from keys above\n",
        "PROMPTS = [plain_caption_dict[i] for i in SELECTED_CMDS]\n",
        "\n",
        "tokens = tokenizer(PROMPTS, padding='max_length', truncation=True, max_length=clip_max_length, return_tensors='pt')\n",
        "input_ids = tokens['input_ids'].to(DEVICE)\n",
        "attention_mask = tokens['attention_mask'].to(DEVICE)\n",
        "attention_mask_bool = attention_mask.to(dtype=torch.bool)\n",
        "with torch.no_grad():\n",
        "    outputs = text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "text_tokens_prompt = outputs.last_hidden_state\n",
        "\n",
        "# Repeat frames for each prompt\n",
        "frames_prompt = frames.repeat(len(PROMPTS), 1, 1, 1, 1)\n",
        "preds_prompt = rollout_predictions(\n",
        "    model_lang,\n",
        "    frames_prompt,\n",
        "    ROLLOUT_STEPS,\n",
        "    text_tokens=text_tokens_prompt,\n",
        "    text_mask=attention_mask_bool,\n",
        ")\n",
        "\n",
        "# Visualize multi-step outputs per prompt\n",
        "rows = len(PROMPTS)\n",
        "cols = ROLLOUT_STEPS\n",
        "plt.figure(figsize=(3 * cols, 3 * rows))\n",
        "for i, prompt in enumerate(PROMPTS):\n",
        "    for t in range(ROLLOUT_STEPS):\n",
        "        rgb = decode_feats(preds_prompt[i:i+1, t])[0]\n",
        "        ax = plt.subplot(rows, cols, i * cols + t + 1)\n",
        "        ax.imshow(rgb.permute(1, 2, 0))\n",
        "        if t == 0:\n",
        "            ax.set_ylabel(prompt, rotation=0, labelpad=40, fontsize=10)\n",
        "        ax.set_title(f't{t+1}')\n",
        "        ax.axis('off')\n",
        "plt.tight_layout()\n",
        "\n",
        "# Quick feature-diff sanity check between first two prompts\n",
        "if len(PROMPTS) >= 2:\n",
        "    preds_a = rollout_predictions(model_lang, frames, 1, text_tokens=text_tokens_prompt[:1], text_mask=attention_mask_bool[:1])\n",
        "    preds_b = rollout_predictions(model_lang, frames, 1, text_tokens=text_tokens_prompt[1:2], text_mask=attention_mask_bool[1:2])\n",
        "    print('feat diff (step1):', (preds_a - preds_b).abs().mean().item())\n",
        "else:\n",
        "    print('Need >=2 prompts for feature-diff check')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes / Troubleshooting\n",
        "- If you see GitHub access errors for DINOv2, set `DINO_REPO` to a local cache path.\n",
        "- If `lpips` is not installed, pass `lpips_weight=0` when loading the decoder.\n",
        "- If `text_tokens` is None, ensure `opendv_use_lang_annos` and `opendv_use_lang_features` are enabled and your language features are available."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
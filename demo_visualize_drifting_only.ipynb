{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Drifting Model (Single Checkpoint)\n",
    "\n",
    "This notebook visualizes a **single drifting model** (no baseline comparison):\n",
    "- one-step predictions on the same input with multiple noise samples\n",
    "- diversity maps (feature/RGB std)\n",
    "- drift vector norm heatmap on one sample (if GT next frame is available)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.data import OpenDV_VideoData\n",
    "from src.dino_f import Dino_f\n",
    "from src.drifting_utils import compute_V, build_token_sample_ids\n",
    "from save_predicted_dino_features import add_missing_args, denormalize_images\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('DEVICE =', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b13676c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Config =====\n",
    "DINO_F_CKPT = '/cpfs/pengyu/DINO-Foresight/logs/dino_foresight_lowres_opendv_drift_noallgather/20260215_011651/checkpoints/epoch=9-step=535547.ckpt'\n",
    "\n",
    "OPENDV_ROOT = '/pfs/pengyu/OpenDV-YouTube'\n",
    "OPENDV_LANG_ROOT = '/pfs/pengyu/OpenDV-YouTube-Language'\n",
    "LANG_CACHE_VAL = '/pfs/pengyu/OpenDV-YouTube-Language/mini_val_cache.json'\n",
    "OPENDV_LANG_FEAT_NAME = 'lang_clip_{start}_{end}.pt'\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "NUM_WORKERS = 2\n",
    "NUM_NOISE_SAMPLES = 6  # number of stochastic one-step predictions on the same input\n",
    "ROLLOUT_STEPS = 6      # optional long rollout visualization\n",
    "OPENDV_RETURN_FUTURE_GT = True\n",
    "OPENDV_EVAL_FUTURE_STEPS = max(0, ROLLOUT_STEPS - 1)\n",
    "\n",
    "# Optional decoder for RGB visualization (set to None to visualize feature maps only)\n",
    "DECODER_CKPT = '/cpfs/pengyu/DINO-Foresight/dino-foresight/j5ludt8t/checkpoints/epoch=20-step=189609.ckpt'\n",
    "DECODER_TYPE = 'from_dino'  # 'from_dino' or 'from_feats'\n",
    "\n",
    "# Optional: local DINOv2 hub cache\n",
    "os.environ.setdefault('DINO_REPO', '/cpfs/pengyu/.cache/torch/hub/facebookresearch_dinov2_main')\n",
    "\n",
    "assert Path(DINO_F_CKPT).exists(), f'Missing checkpoint: {DINO_F_CKPT}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b336c378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load drifting model\n",
    "model = Dino_f.load_from_checkpoint(DINO_F_CKPT, strict=False, map_location='cpu').to(DEVICE)\n",
    "model.eval()\n",
    "model._init_feature_extractor()\n",
    "if model.dino_v2 is not None:\n",
    "    model.dino_v2 = model.dino_v2.to(DEVICE)\n",
    "if model.eva2clip is not None:\n",
    "    model.eva2clip = model.eva2clip.to(DEVICE)\n",
    "if model.sam is not None:\n",
    "    model.sam = model.sam.to(DEVICE)\n",
    "\n",
    "print('use_drifting_loss:', getattr(model.args, 'use_drifting_loss', False))\n",
    "print('drift_temperatures:', getattr(model.args, 'drift_temperatures', None))\n",
    "print('drift_step_size:', getattr(model.args, 'drift_step_size', None))\n",
    "print('drift_anchor_weight:', getattr(model.args, 'drift_anchor_weight', None))\n",
    "print('feature_extractor:', model.args.feature_extractor)\n",
    "print('img_size:', model.args.img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c585b96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build OpenDV val loader aligned to this checkpoint\n",
    "use_lang_cond = bool(getattr(model.args, 'use_language_condition', False))\n",
    "use_precomputed_text = bool(getattr(model.args, 'use_precomputed_text', False))\n",
    "\n",
    "args = SimpleNamespace(\n",
    "    data_path=OPENDV_ROOT,\n",
    "    opendv_root=OPENDV_ROOT,\n",
    "    opendv_lang_root=OPENDV_LANG_ROOT,\n",
    "    opendv_use_lang_annos=bool(OPENDV_LANG_ROOT),\n",
    "    opendv_lang_cache_train=None,\n",
    "    opendv_lang_cache_val=LANG_CACHE_VAL,\n",
    "    opendv_use_lang_features=bool(use_lang_cond and use_precomputed_text),\n",
    "    opendv_return_language=bool(use_lang_cond and (not use_precomputed_text)),\n",
    "    opendv_lang_feat_name=OPENDV_LANG_FEAT_NAME,\n",
    "    opendv_video_dir=None,\n",
    "    opendv_max_clips=None,\n",
    "    sequence_length=getattr(model.args, 'sequence_length', 5),\n",
    "    img_size=tuple(getattr(model.args, 'img_size', (224, 448))),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    num_workers_val=NUM_WORKERS,\n",
    "    eval_mode=True,\n",
    "    eval_midterm=False,\n",
    "    eval_modality=None,\n",
    "    use_language_condition=use_lang_cond,\n",
    "    use_precomputed_text=use_precomputed_text,\n",
    "    opendv_return_future_gt=OPENDV_RETURN_FUTURE_GT,\n",
    "    opendv_eval_future_steps=OPENDV_EVAL_FUTURE_STEPS,\n",
    ")\n",
    "args = add_missing_args(args, model.args)\n",
    "args.feature_extractor = model.args.feature_extractor\n",
    "args.dinov2_variant = getattr(model.args, 'dinov2_variant', 'vitb14_reg')\n",
    "args.return_rgb_path = True\n",
    "\n",
    "data = OpenDV_VideoData(arguments=args, subset='val', batch_size=BATCH_SIZE)\n",
    "loader = data.val_dataloader()\n",
    "batch = next(iter(loader))\n",
    "print('Loaded one val batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135d6a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_batch(batch):\n",
    "    if not isinstance(batch, (list, tuple)):\n",
    "        return batch, None, None, None, None, None\n",
    "\n",
    "    frames = batch[0]\n",
    "    gt_img = None\n",
    "    text_tokens = None\n",
    "    text_mask = None\n",
    "    future_gt = None\n",
    "    rgb_paths = None\n",
    "\n",
    "    for item in batch[1:]:\n",
    "        if torch.is_tensor(item):\n",
    "            if item.ndim == 4 and item.shape[1] == 3:\n",
    "                gt_img = item\n",
    "            elif item.ndim == 3:\n",
    "                text_tokens = item\n",
    "            elif item.ndim == 2:\n",
    "                text_mask = item\n",
    "            elif item.ndim == 5 and item.shape[2] == 3:\n",
    "                future_gt = item\n",
    "        elif isinstance(item, (list, tuple)) and len(item) > 0 and isinstance(item[0], (str, Path)):\n",
    "            rgb_paths = [str(p) for p in item]\n",
    "\n",
    "    return frames, gt_img, text_tokens, text_mask, future_gt, rgb_paths\n",
    "\n",
    "\n",
    "def prepare_text_tokens(model, text_tokens, text_mask, device):\n",
    "    if not getattr(model, 'use_language_condition', False):\n",
    "        return None, None\n",
    "    if text_tokens is None:\n",
    "        return None, None\n",
    "\n",
    "    tt = text_tokens.to(device)\n",
    "    tm = text_mask.to(device) if text_mask is not None else None\n",
    "\n",
    "    if hasattr(model, 'text_proj'):\n",
    "        in_dim = model.text_proj.in_features\n",
    "        out_dim = model.text_proj.out_features\n",
    "        if tt.shape[-1] == in_dim:\n",
    "            tt = tt.to(dtype=model.text_proj.weight.dtype)\n",
    "            tt = model.text_proj(tt)\n",
    "        elif tt.shape[-1] != out_dim:\n",
    "            raise ValueError(f'Unexpected text token dim {tt.shape[-1]} (expected {in_dim} or {out_dim})')\n",
    "\n",
    "    return tt, tm\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def one_step_pred_feats(model, frames, text_tokens=None, text_mask=None):\n",
    "    x = model.preprocess(frames)\n",
    "    tt, tm = prepare_text_tokens(model, text_tokens, text_mask, x.device)\n",
    "\n",
    "    masked_x, mask = model.get_mask_tokens(x, mode='full_mask', mask_frames=1)\n",
    "    mask = mask.to(x.device)\n",
    "\n",
    "    if model.args.vis_attn:\n",
    "        _, x_pred, _ = model.forward(x, masked_x, mask, text_tokens=tt, text_mask=tm)\n",
    "    else:\n",
    "        _, x_pred = model.forward(x, masked_x, mask, text_tokens=tt, text_mask=tm)\n",
    "\n",
    "    x_pred = model.postprocess(x_pred)\n",
    "    return x_pred[:, -1], x, tt, tm  # [B,Hf,Wf,C], plus internals\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def rollout_pred_feats(model, frames, steps, text_tokens=None, text_mask=None):\n",
    "    x = model.preprocess(frames)\n",
    "    tt, tm = prepare_text_tokens(model, text_tokens, text_mask, x.device)\n",
    "    outs = []\n",
    "    for _ in range(steps):\n",
    "        masked_x, mask = model.get_mask_tokens(x, mode='full_mask', mask_frames=1)\n",
    "        mask = mask.to(x.device)\n",
    "        if model.args.vis_attn:\n",
    "            _, x_pred_latent, _ = model.forward(x, masked_x, mask, text_tokens=tt, text_mask=tm)\n",
    "        else:\n",
    "            _, x_pred_latent = model.forward(x, masked_x, mask, text_tokens=tt, text_mask=tm)\n",
    "\n",
    "        # Recurrence must stay in preprocess(latent) space.\n",
    "        x[:, :-1] = x[:, 1:].clone()\n",
    "        x[:, -1] = x_pred_latent[:, -1]\n",
    "\n",
    "        # Export in original feature space for decoder/visualization.\n",
    "        x_pred_post = model.postprocess(x_pred_latent)\n",
    "        outs.append(x_pred_post[:, -1].detach().cpu())\n",
    "\n",
    "    return torch.stack(outs, dim=1)\n",
    "\n",
    "\n",
    "def gt_next_feats(model, gt_img, h, w):\n",
    "    with torch.no_grad():\n",
    "        feats = model.extract_features(gt_img)\n",
    "        feats = feats.reshape(feats.shape[0], h, w, -1)\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3df0d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional decoder\n",
    "decode_feats = None\n",
    "if DECODER_CKPT is not None:\n",
    "    if DECODER_TYPE == 'from_feats':\n",
    "        from train_rgb_decoder_from_feats import FeatureRgbDecoder\n",
    "        decoder = FeatureRgbDecoder.load_from_checkpoint(DECODER_CKPT, strict=False).to(DEVICE)\n",
    "        decoder.eval()\n",
    "        def decode_feats(feats_bhwc):\n",
    "            return decoder(feats_bhwc.to(DEVICE)).detach().cpu()\n",
    "    elif DECODER_TYPE == 'from_dino':\n",
    "        from train_rgb_decoder import DinoV2RGBDecoder\n",
    "        decoder = DinoV2RGBDecoder.load_from_checkpoint(DECODER_CKPT, strict=False, lpips_weight=0).to(DEVICE)\n",
    "        decoder.eval()\n",
    "        dpt_head = decoder.decoder\n",
    "        feat_dim = decoder.emb_dim\n",
    "\n",
    "        def decode_feats(feats_bhwc):\n",
    "            feats_bhwc = feats_bhwc.to(DEVICE)\n",
    "            b, h, w, c = feats_bhwc.shape\n",
    "            if c % feat_dim != 0:\n",
    "                raise ValueError(f'Feature dim mismatch: {c} not divisible by {feat_dim}')\n",
    "            parts = torch.split(feats_bhwc, feat_dim, dim=-1)\n",
    "            if len(parts) == 2:\n",
    "                parts = [parts[0], parts[0], parts[1], parts[1]]\n",
    "            elif len(parts) != 4:\n",
    "                raise ValueError(f'Expected 2 or 4 feature chunks, got {len(parts)}')\n",
    "            feat_list = [p.reshape(b, h * w, feat_dim) for p in parts]\n",
    "            pred = dpt_head(feat_list, h, w)\n",
    "            pred = F.interpolate(pred, size=tuple(model.args.img_size), mode='bicubic', align_corners=False)\n",
    "            pred = torch.sigmoid(pred)\n",
    "            return pred.detach().cpu()\n",
    "    else:\n",
    "        raise ValueError(\"DECODER_TYPE must be 'from_dino' or 'from_feats'\")\n",
    "\n",
    "print('decoder enabled:', decode_feats is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0196c7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multiple one-step stochastic samples on the same input\n",
    "frames, gt_img, text_tokens, text_mask, future_gt, rgb_paths = parse_batch(batch)\n",
    "frames = frames.to(DEVICE)\n",
    "if gt_img is not None:\n",
    "    gt_img = gt_img.to(DEVICE)\n",
    "if future_gt is not None:\n",
    "    future_gt = future_gt.to(DEVICE)\n",
    "    print('future_gt shape:', tuple(future_gt.shape))\n",
    "\n",
    "pred_list = []\n",
    "x_internal = None\n",
    "for k in range(NUM_NOISE_SAMPLES):\n",
    "    pred_k, x_internal, tt_internal, tm_internal = one_step_pred_feats(\n",
    "        model, frames, text_tokens=text_tokens, text_mask=text_mask\n",
    "    )\n",
    "    pred_list.append(pred_k.detach().cpu())\n",
    "\n",
    "pred_stack = torch.stack(pred_list, dim=1)  # [B, K, Hf, Wf, C]\n",
    "print('pred_stack shape:', tuple(pred_stack.shape))\n",
    "\n",
    "# Diversity in feature space\n",
    "feat_std_map = pred_stack[0].std(dim=0).norm(dim=-1)  # [Hf, Wf]\n",
    "print('feature std mean:', float(feat_std_map.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cc50a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: context/gt + sampled predictions + diversity map\n",
    "ctx = denormalize_images(frames[0].detach().cpu(), model.args.feature_extractor).cpu()\n",
    "ctx_last = ctx[-1]\n",
    "\n",
    "if decode_feats is not None:\n",
    "    pred_rgbs = [decode_feats(pred_stack[:, i])[0].clamp(0, 1) for i in range(pred_stack.shape[1])]\n",
    "\n",
    "    show = [ctx_last]\n",
    "    titles = ['context last']\n",
    "\n",
    "    if gt_img is not None:\n",
    "        gt_rgb = denormalize_images(gt_img.detach().cpu(), model.args.feature_extractor).cpu()[0]\n",
    "        show.append(gt_rgb)\n",
    "        titles.append('gt next')\n",
    "\n",
    "    show.extend(pred_rgbs)\n",
    "    titles.extend([f'sample {i+1}' for i in range(len(pred_rgbs))])\n",
    "\n",
    "    plt.figure(figsize=(3 * len(show), 3.5))\n",
    "    for i, (im, tt) in enumerate(zip(show, titles), start=1):\n",
    "        ax = plt.subplot(1, len(show), i)\n",
    "        ax.imshow(im.permute(1, 2, 0))\n",
    "        ax.set_title(tt)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    rgb_stack = torch.stack(pred_rgbs, dim=0)  # [K,3,H,W]\n",
    "    rgb_std = rgb_stack.std(dim=0).mean(dim=0)  # [H,W]\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(rgb_std, cmap='magma')\n",
    "    plt.title('RGB std map across noise samples')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "else:\n",
    "    # No decoder: show feature-space maps only\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(ctx_last.permute(1, 2, 0))\n",
    "    plt.title('context last')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(feat_std_map, cmap='magma')\n",
    "    plt.title('feature std map across noise samples')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6eaa340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image reconstruction metrics (one-step, across noise samples)\n",
    "if decode_feats is None:\n",
    "    print('decoder is None: cannot compute image reconstruction metrics')\n",
    "elif gt_img is None:\n",
    "    print('gt_img is None: cannot compute image reconstruction metrics')\n",
    "else:\n",
    "    gt_rgb = denormalize_images(gt_img.detach().cpu(), model.args.feature_extractor).cpu()[0].clamp(0, 1)\n",
    "\n",
    "    recon_list = []\n",
    "    for i in range(pred_stack.shape[1]):\n",
    "        recon = decode_feats(pred_stack[:, i])[0].clamp(0, 1).cpu()\n",
    "        recon_list.append(recon)\n",
    "    recon_stack = torch.stack(recon_list, dim=0)  # [K,3,H,W]\n",
    "\n",
    "    mse = ((recon_stack - gt_rgb.unsqueeze(0)) ** 2).mean(dim=(1, 2, 3))\n",
    "    l1 = (recon_stack - gt_rgb.unsqueeze(0)).abs().mean(dim=(1, 2, 3))\n",
    "    psnr = 10.0 * torch.log10(1.0 / mse.clamp_min(1e-12))\n",
    "\n",
    "    ssim_scores = None\n",
    "    try:\n",
    "        from torchmetrics.functional import structural_similarity_index_measure as ssim\n",
    "        vals = []\n",
    "        for i in range(recon_stack.shape[0]):\n",
    "            vals.append(ssim(recon_stack[i:i+1], gt_rgb.unsqueeze(0), data_range=1.0).item())\n",
    "        ssim_scores = torch.tensor(vals)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print('Per-sample one-step reconstruction metrics:')\n",
    "    for i in range(recon_stack.shape[0]):\n",
    "        msg = f'sample {i+1:02d} | MSE={mse[i].item():.6f} | L1={l1[i].item():.6f} | PSNR={psnr[i].item():.3f} dB'\n",
    "        if ssim_scores is not None:\n",
    "            msg += f' | SSIM={ssim_scores[i].item():.4f}'\n",
    "        print(msg)\n",
    "\n",
    "    print('--- summary over noise samples ---')\n",
    "    print(f'MSE  mean={mse.mean().item():.6f} std={mse.std(unbiased=False).item():.6f}')\n",
    "    print(f'L1   mean={l1.mean().item():.6f} std={l1.std(unbiased=False).item():.6f}')\n",
    "    print(f'PSNR mean={psnr.mean().item():.3f} std={psnr.std(unbiased=False).item():.3f} dB')\n",
    "    if ssim_scores is not None:\n",
    "        print(f'SSIM mean={ssim_scores.mean().item():.4f} std={ssim_scores.std(unbiased=False).item():.4f}')\n",
    "    else:\n",
    "        print('SSIM unavailable (torchmetrics missing)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1951ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drift vector norm heatmap on one sample (requires gt_img)\n",
    "if gt_img is None:\n",
    "    print('gt_img is None: skip V heatmap')\n",
    "else:\n",
    "    # use the first sampled pred as x\n",
    "    x_pred = pred_stack[:, 0].to(DEVICE)  # [B,Hf,Wf,C]\n",
    "    B, Hf, Wf, C = x_pred.shape\n",
    "    n_tok = Hf * Wf\n",
    "\n",
    "    # gt feature\n",
    "    gt_f = gt_next_feats(model, gt_img, Hf, Wf).to(DEVICE)\n",
    "\n",
    "    x_tokens = x_pred.reshape(B * n_tok, C).float()\n",
    "    y_tokens = gt_f.reshape(B * n_tok, C).float()\n",
    "\n",
    "    sample_ids = build_token_sample_ids(B, n_tok, x_tokens.device)\n",
    "    V = compute_V(\n",
    "        x=x_tokens,\n",
    "        y_pos=y_tokens,\n",
    "        y_neg=x_tokens,\n",
    "        temperatures=getattr(model, 'drift_temperatures', (0.02, 0.05, 0.2)),\n",
    "        x_sample_ids=sample_ids,\n",
    "        y_pos_sample_ids=sample_ids,\n",
    "        y_neg_sample_ids=sample_ids,\n",
    "    )\n",
    "\n",
    "    vnorm = V.norm(dim=-1).reshape(B, Hf, Wf)[0].detach().cpu()\n",
    "\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(vnorm, cmap='magma')\n",
    "    plt.title('||V|| heatmap (token space)')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bf2260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional rollout visualization (single model) + GT comparison\n",
    "pred_roll = rollout_pred_feats(model, frames, ROLLOUT_STEPS, text_tokens=text_tokens, text_mask=text_mask)\n",
    "print('pred_roll shape:', tuple(pred_roll.shape))\n",
    "\n",
    "if decode_feats is None:\n",
    "    print('Set DECODER_CKPT to visualize rollout RGBs.')\n",
    "else:\n",
    "    pred_roll_rgb = [decode_feats(pred_roll[:, t])[0].clamp(0, 1) for t in range(ROLLOUT_STEPS)]\n",
    "\n",
    "    if gt_img is None:\n",
    "        print('gt_img is None: show rollout only (no GT comparison).')\n",
    "        plt.figure(figsize=(3 * ROLLOUT_STEPS, 3))\n",
    "        for t in range(ROLLOUT_STEPS):\n",
    "            ax = plt.subplot(1, ROLLOUT_STEPS, t + 1)\n",
    "            ax.imshow(pred_roll_rgb[t].permute(1, 2, 0))\n",
    "            ax.set_title(f'pred t+{t+1}')\n",
    "            ax.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        # Build GT rollout list: [gt_next] + future_gt[:]\n",
    "        gt_roll_rgb = [denormalize_images(gt_img.detach().cpu(), model.args.feature_extractor).cpu()[0].clamp(0, 1)]\n",
    "        if future_gt is not None and future_gt.shape[1] > 0:\n",
    "            future_rgb = denormalize_images(future_gt[0].detach().cpu(), model.args.feature_extractor).cpu().clamp(0, 1)\n",
    "            for i in range(future_rgb.shape[0]):\n",
    "                gt_roll_rgb.append(future_rgb[i])\n",
    "\n",
    "        n_compare = min(ROLLOUT_STEPS, len(gt_roll_rgb))\n",
    "        if n_compare < ROLLOUT_STEPS:\n",
    "            print(f'Only {n_compare} GT steps available for comparison.')\n",
    "\n",
    "        plt.figure(figsize=(3 * ROLLOUT_STEPS, 6))\n",
    "        for t in range(ROLLOUT_STEPS):\n",
    "            ax1 = plt.subplot(2, ROLLOUT_STEPS, t + 1)\n",
    "            if t < len(gt_roll_rgb):\n",
    "                ax1.imshow(gt_roll_rgb[t].permute(1, 2, 0))\n",
    "                ax1.set_title(f'GT t+{t+1}')\n",
    "            else:\n",
    "                ax1.text(0.5, 0.5, 'N/A', ha='center', va='center')\n",
    "                ax1.set_title(f'GT t+{t+1}')\n",
    "            ax1.axis('off')\n",
    "\n",
    "            ax2 = plt.subplot(2, ROLLOUT_STEPS, ROLLOUT_STEPS + t + 1)\n",
    "            ax2.imshow(pred_roll_rgb[t].permute(1, 2, 0))\n",
    "            ax2.set_title(f'pred t+{t+1}')\n",
    "            ax2.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Quantitative metrics for available GT steps.\n",
    "        print('Per-step rollout metrics:')\n",
    "        for t in range(n_compare):\n",
    "            pred_t = pred_roll_rgb[t]\n",
    "            gt_t = gt_roll_rgb[t]\n",
    "            mse = torch.mean((pred_t - gt_t) ** 2).item()\n",
    "            l1 = torch.mean(torch.abs(pred_t - gt_t)).item()\n",
    "            psnr = 10.0 * torch.log10(1.0 / torch.tensor(max(mse, 1e-12))).item()\n",
    "\n",
    "            ssim_val = None\n",
    "            try:\n",
    "                from torchmetrics.functional import structural_similarity_index_measure as ssim\n",
    "                ssim_val = ssim(pred_t.unsqueeze(0), gt_t.unsqueeze(0), data_range=1.0).item()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            msg = f't+{t+1} | MSE={mse:.6f} | L1={l1:.6f} | PSNR={psnr:.3f} dB'\n",
    "            if ssim_val is not None:\n",
    "                msg += f' | SSIM={ssim_val:.4f}'\n",
    "            print(msg)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
